# Experience Management

Understanding how users experience services—whether they can achieve what they need, how they feel about the interaction, and where friction or failure occurs—so that delivery can be improved from the user’s perspective. Organizations use experience data to complement availability and incident metrics with outcomes and satisfaction.

## Primary Objects/Actors

- **Experience** — The user’s encounter with a service: the end-to-end journey (e.g., requesting access, using the tool, getting help). Experience is the thing being observed and improved; it may be captured as feedback, journey maps, or metrics (e.g., task completion, satisfaction scores).
- **User / Consumer** — The person or group who uses the service. They are the primary source of experience data (feedback, surveys, support contacts) and the primary beneficiary of improvements.

## Supporting Objects/Actors

- **Feedback** — Direct input from users: surveys, comments, support tickets, or interviews. Used to identify pain points, satisfaction levels, and unmet needs.
- **Journey / Touchpoint** — A step or moment in the user’s interaction with the service (e.g., "request access," "first login," "report an issue"). Mapping touchpoints helps locate where experience breaks down or excels.

## Related Objects/Actors

- **Service** — Experience is measured and improved per service (or per channel). The service definition describes what is being consumed; experience management describes how it is experienced.
- **Incident** — Incidents often represent a bad experience (something broke). Experience data may include incident frequency and resolution quality as seen by the user.
- **Service channel** — How the user reaches the service (portal, phone, chat, etc.) affects experience; experience management may be combined with service channel selection to improve both what is offered and how it is delivered.

## Lifecycles

Experience is not usually tracked with a single lifecycle like incidents or changes; instead, the *practice* of experience management follows a cycle:

1. **Capture** — Gather experience data: surveys, feedback, support themes, journey observations.
2. **Analyze** — Identify patterns: where do users struggle? What do they value? How does experience vary by segment or channel?
3. **Act** — Prioritize and implement improvements (process, tool, communication, or service change). Some actions may feed into incident, problem, or change coordination.
4. **Measure** — Track whether experience improves (e.g., satisfaction, task completion, repeat contacts). Repeat the cycle.

Organizations may run this cycle per service, per channel, or per initiative; the rhythm (e.g., quarterly surveys, continuous feedback) is set locally.

## Getting Started

- **Minimal template:** For at least one service (or channel), choose one way to capture experience: e.g., a short survey after support contact, a periodic "how are we doing?" survey, or a lightweight review of support themes and repeat contacts. Record the results in a single place (spreadsheet, dashboard, or tool) and review them on a regular cadence (e.g., monthly) with the service owner or team. Decide on one or two actions per cycle.
- **When to introduce:** Introduce when availability and incident metrics look good but users are still frustrated; when leadership asks "are our users happy?" and no one has data; or when you want to prioritize improvements by impact on the user. Pain signals include "we think we’re doing well but we don’t know," high repeat contacts or escalations, and improvement efforts that don’t target real pain points.

## Processes

- **Capturing experience:** Organizations use surveys (post-contact or periodic), feedback channels (comments, ratings), support ticket themes, and sometimes interviews or journey workshops. Start with one channel (e.g., post-incident survey or quarterly satisfaction) and add others as capacity allows.
- **Analyzing experience:** Feedback and metrics are summarized: satisfaction scores, common themes, failure points in journeys. Analysis may be done by the service owner, a dedicated role, or a cross-functional group. Output is a short set of insights and recommended actions.
- **Acting on experience:** Insights are turned into actions: process changes, tool improvements, communication, or service changes. Some actions are handled in incident or problem management (e.g., fix a recurring issue); others are projects or backlog items. Ownership is assigned so that actions are completed.
- **Measuring improvement:** Over time, the organization tracks whether experience metrics improve (satisfaction, task completion, repeat contacts). The cycle repeats so that experience management is continuous.

## Adaptation

- **What to measure:** Organizations choose metrics that matter: satisfaction (e.g., NPS, CSAT), task completion, time-to-outcome, or qualitative themes. Start with one or two metrics that you will actually act on; avoid collecting experience data that no one uses.
- **Who owns experience:** In some organizations the service owner owns experience; in others a dedicated experience or support function leads capture and analysis and works with service owners on actions. Choose a model that fits structure and capacity.
- **Pitfalls:** Surveying without closing the loop (no visible action); measuring experience in isolation from incident and service data so that "experience" and "reliability" are disconnected; or aiming for perfect measurement before taking any action. Prefer simple, actionable feedback and a regular cycle of act-and-measure over elaborate programs that never change behavior.
